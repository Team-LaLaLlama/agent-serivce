import os
import asyncio
import json
import glob
import torch
import chromadb
import requests
from datetime import datetime
from dotenv import load_dotenv
from crewai import Agent, Task, Crew
from crewai.llm import LLM
from bs4 import BeautifulSoup

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

load_dotenv()

# =================================================================
# 1. RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ë° í•¨ìˆ˜ ì •ì˜
# =================================================================

# --- ê²½ë¡œ ì„¤ì • ---
PROPOSAL_DIR = "./proposal"
RFP_PATH = "./RFP/ìˆ˜í˜‘_rfp.pdf"
OUTPUT_DIR = "./output"
EVALUATION_CRITERIA_PATH = "./standard/evaluation_criteria.md"
CHROMA_PERSIST_DIR = "./chroma_db_html_parsed"
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")

# --- ì „ì—­ ë³€ìˆ˜ ---
unified_vectorstore = None

# =================================================================
# HTML ì²­í‚¹ í•¨ìˆ˜ (RAG_pipeline.ipynbì—ì„œ ê°€ì ¸ì˜´)
# =================================================================

def split_text_by_length(text, max_length, overlap):
    """í…ìŠ¤íŠ¸ê°€ ê¸¸ ê²½ìš° ì§€ì •ëœ í¬ê¸°ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    if len(text) <= max_length:
        return [text]
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_length
        chunks.append(text[start:end])
        start += max_length - overlap
    return chunks

def chunk_html_recursively(html_content, proposal_id, max_chunk_size=1000, chunk_overlap=100):
    """HTMLì„ ì œëª©(h íƒœê·¸) ê³„ì¸µ êµ¬ì¡°ì— ë”°ë¼ ì•ˆì •ì ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(html_content, 'html.parser')
    chunks = []
    chunk_index = 0
    
    header_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']
    all_headers = soup.find_all(header_tags)

    # í—¤ë”ê°€ ì‹œì‘ë˜ê¸° ì „ì˜ í…ìŠ¤íŠ¸ë„ ì²˜ë¦¬
    if all_headers:
        first_header = all_headers[0]
        text_before_first_header = ""
        for elem in first_header.find_previous_siblings():
            text_before_first_header = elem.get_text(separator=' ', strip=True) + " " + text_before_first_header
        
        if text_before_first_header.strip():
            text_chunks = split_text_by_length(text_before_first_header.strip(), max_chunk_size, chunk_overlap)
            for text_chunk in text_chunks:
                chunks.append({
                    "proposal_id": proposal_id,
                    "source_id": f"{proposal_id}_chunk_{chunk_index}",
                    "heading_context": "ì„œë¬¸",
                    "original_text": text_chunk
                })
                chunk_index += 1

    # í—¤ë” ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ ë¶„í• 
    for i, header in enumerate(all_headers):
        section_text_parts = [header.get_text(separator=' ', strip=True)]
        
        for sibling in header.find_next_siblings():
            if sibling.name in header_tags:
                break
            section_text_parts.append(sibling.get_text(separator=' ', strip=True))

        full_section_text = ' '.join(filter(None, section_text_parts))
        if not full_section_text.strip():
            continue

        parent_headers = [h.get_text(strip=True) for h in header.find_parents(header_tags)]
        parent_headers.reverse()
        heading_context = parent_headers + [header.get_text(strip=True)]

        text_chunks = split_text_by_length(full_section_text, max_chunk_size, chunk_overlap)
        for text_chunk in text_chunks:
            chunks.append({
                "proposal_id": proposal_id,
                "source_id": f"{proposal_id}_chunk_{chunk_index}",
                "heading_context": " > ".join(heading_context),
                "original_text": text_chunk
            })
            chunk_index += 1
            
    # í—¤ë”ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš° body ì „ì²´ë¥¼ ì²˜ë¦¬
    if not all_headers and soup.body:
        full_text = soup.body.get_text(separator=' ', strip=True)
        if full_text:
            text_chunks = split_text_by_length(full_text, max_chunk_size, chunk_overlap)
            for text_chunk in text_chunks:
                chunks.append({
                    "proposal_id": proposal_id,
                    "source_id": f"{proposal_id}_chunk_{chunk_index}",
                    "heading_context": "ë³¸ë¬¸",
                    "original_text": text_chunk
                })
                chunk_index += 1
                
    return chunks

# =================================================================
# RAG ì´ˆê¸°í™” í•¨ìˆ˜
# =================================================================

def initialize_rag_components():
    """RAGì— í•„ìš”í•œ ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if torch.cuda.is_available():
        device = "cuda"
        torch.cuda.set_device(0)
        print(f"INFO: CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        print("WARNING: CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    print("INFO: ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    print("INFO: ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ.")
    return embedding_model

def create_unified_vectorstore(proposal_files, rfp_path, embedding_model):
    """ëª¨ë“  ì œì•ˆì„œì™€ RFP ë¬¸ì„œë¥¼ ë‹¨ì¼ ë²¡í„° ìŠ¤í† ì–´ë¡œ ë³€í™˜"""
    print(f"\n--- [RAG Setup] í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    
    all_chunked_data = []
    
    # 1. RFP(PDF) ì²˜ë¦¬ - Upstage API ì‚¬ìš©
    print("\n[RFP ì²˜ë¦¬]")
    try:
        rfp_id = os.path.splitext(os.path.basename(rfp_path))[0]
        url = "https://api.upstage.ai/v1/document-digitization"
        headers = {"Authorization": f"Bearer {UPSTAGE_API_KEY}"}
        
        with open(rfp_path, "rb") as f:
            files = {"document": f}
            data = {"ocr": "force", "model": "document-parse"}
            response = requests.post(url, headers=headers, files=files, data=data)
            response.raise_for_status()
            html_from_api = response.json().get("content", {}).get("html", "")
            
        if html_from_api:
            rfp_chunks = chunk_html_recursively(html_from_api, rfp_id)
            all_chunked_data.extend(rfp_chunks)
            print(f"  âœ“ RFP '{rfp_id}' ì²˜ë¦¬ ì™„ë£Œ: {len(rfp_chunks)}ê°œ ì²­í¬ ìƒì„±")
    except Exception as e:
        print(f"  âœ— RFP ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 2. ì œì•ˆì„œ(HTML) ì²˜ë¦¬
    print("\n[ì œì•ˆì„œ ì²˜ë¦¬]")
    for file_path in proposal_files:
        try:
            proposal_id = os.path.splitext(os.path.basename(file_path))[0]
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            proposal_chunks = chunk_html_recursively(html_content, proposal_id)
            all_chunked_data.extend(proposal_chunks)
            print(f"  âœ“ ì œì•ˆì„œ '{proposal_id}' ì²˜ë¦¬ ì™„ë£Œ: {len(proposal_chunks)}ê°œ ì²­í¬ ìƒì„±")
        except Exception as e:
            print(f"  âœ— ì œì•ˆì„œ '{os.path.basename(file_path)}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    if not all_chunked_data:
        print("\në²¡í„° DBì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # 3. Document ê°ì²´ë¡œ ë³€í™˜
    documents = [
        Document(
            page_content=chunk["original_text"],
            metadata={
                "proposal_id": chunk["proposal_id"],
                "source_id": chunk["source_id"],
                "heading_context": chunk["heading_context"]
            }
        ) for chunk in all_chunked_data
    ]
    print(f"\nâœ“ ì´ {len(documents)}ê°œì˜ ì²­í¬ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")

    # 4. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    print("\n[ë²¡í„°í™” ë° ì €ì¥]")
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embedding_model,
        persist_directory=CHROMA_PERSIST_DIR
    )
    print(f"  âœ“ í†µí•© ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ! ({CHROMA_PERSIST_DIR})")
    return vectorstore

def get_context_for_category(proposal_file, category_keywords):
    """ëŒ€ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    global unified_vectorstore
    if unified_vectorstore is None:
        return "ì˜¤ë¥˜: ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    print(f"INFO: RAG ê²€ìƒ‰ ì‹¤í–‰ -> ì œì•ˆì„œ: '{proposal_file}', ëŒ€ë¶„ë¥˜: '{category_keywords}'")
    
    # ëŒ€ë¶„ë¥˜ ê´€ë ¨ ë‚´ìš©ì„ ë” ë§ì´ ê°€ì ¸ì˜´
    results = unified_vectorstore.similarity_search(
        query=category_keywords,
        k=10,  # ë” ë§ì€ ì²­í¬ë¥¼ ê²€ìƒ‰ (ì œí•œ ì œê±°)
        filter={"proposal_id": proposal_file}
    )
    
    if not results:
        return "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨ (ê¸¸ì´ ì œí•œ ì œê±°)
    context = "\n\n---\n\n".join([doc.page_content for doc in results])
    
    print(f"INFO: ì´ {len(results)}ê°œ ì²­í¬, {len(context)}ìì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    
    return context

def load_evaluation_criteria_as_text(criteria_path):
    """í‰ê°€ ê¸°ì¤€í‘œë¥¼ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë¡œë“œí•©ë‹ˆë‹¤ (íŒŒì‹± ì—†ìŒ)."""
    if not os.path.exists(criteria_path):
        print(f"WARNING: í‰ê°€ ê¸°ì¤€í‘œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {criteria_path}")
        return ""
    
    print(f"INFO: í‰ê°€ ê¸°ì¤€í‘œ ë¡œë”©: {criteria_path}")
    
    with open(criteria_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"INFO: í‰ê°€ ê¸°ì¤€í‘œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤ ({len(content)} ì)")
    return content

def save_evaluation_report(proposal_name, report_content):
    """ì œì•ˆì„œë³„ í‰ê°€ ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    proposal_base_name = os.path.splitext(proposal_name)[0]
    filename = f"{proposal_base_name}_evaluation_report_{timestamp}.txt"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("ì œì•ˆì„œ í‰ê°€ ë³´ê³ ì„œ\n")
        f.write("="*80 + "\n")
        f.write(f"ì œì•ˆì„œëª…: {proposal_name}\n")
        f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("="*80 + "\n")
        f.write("ìµœì¢… í‰ê°€ ë³´ê³ ì„œ\n")
        f.write("="*80 + "\n")
        f.write(report_content)
        f.write("\n\n" + "="*80 + "\n")
        f.write("ë³´ê³ ì„œ ë\n")
        f.write("="*80 + "\n")
    
    print(f"[SAVED] í‰ê°€ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    return filepath

# =================================================================
# LLM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
# =================================================================

def get_llm_model():
    """í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ LLM ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤."""
    model_type = os.getenv('LLM_TYPE', 'local').lower()
    
    if model_type == 'local':
        model_name = os.getenv('LOCAL_MODEL_NAME', 'llama3.2')
        base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        print(f"INFO: ë¡œì»¬ LLM ì‚¬ìš© - ëª¨ë¸: {model_name}, URL: {base_url}")
        return LLM(model=f"ollama/{model_name}", base_url=base_url)
    
    elif model_type == 'huggingface':
        model_name = os.getenv('HF_MODEL_NAME', 'meta-llama/Meta-Llama-3-8B-Instruct')
        api_key = os.getenv('HUGGINGFACEHUB_API_TOKEN')
        if not api_key:
            raise ValueError("HUGGINGFACEHUB_API_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print(f"INFO: HuggingFace LLM ì‚¬ìš© - ëª¨ë¸: {model_name}")
        return LLM(model=f"huggingface/{model_name}", api_key=api_key)
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM íƒ€ì…ì…ë‹ˆë‹¤: {model_type}")

async def main():
    print("## LLM ì£¼ë„í˜• ë™ì  Agent ìƒì„± ë° í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    global unified_vectorstore
    proposal_files = glob.glob(os.path.join(PROPOSAL_DIR, "*.html"))
    
    if not proposal_files or not os.path.exists(RFP_PATH):
        print("ì˜¤ë¥˜: ì œì•ˆì„œ ë˜ëŠ” RFP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
    if os.path.exists(CHROMA_PERSIST_DIR):
        print(f"\nê¸°ì¡´ ë²¡í„° DBë¥¼ '{CHROMA_PERSIST_DIR}' ê²½ë¡œì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
        embedding_model = initialize_rag_components()
        unified_vectorstore = Chroma(
            persist_directory=CHROMA_PERSIST_DIR,
            embedding_function=embedding_model
        )
        print("âœ“ DB ë¡œë“œ ì™„ë£Œ!")
    else:
        embedding_model = initialize_rag_components()
        unified_vectorstore = create_unified_vectorstore(
            proposal_files, RFP_PATH, embedding_model
        )

    # í‰ê°€ ê¸°ì¤€í‘œë¥¼ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë¡œë“œ (íŒŒì‹± ì—†ìŒ)
    evaluation_criteria_text = load_evaluation_criteria_as_text(EVALUATION_CRITERIA_PATH)
    
    if not evaluation_criteria_text:
        print("ERROR: í‰ê°€ ê¸°ì¤€í‘œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # LLM ì´ˆê¸°í™”
    llm = get_llm_model()

    # =================================================================
    # Phase 1: Dispatcherê°€ ë§ˆí¬ë‹¤ìš´ì„ ë¶„ì„í•˜ì—¬ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ë§Œ ì¶”ì¶œ
    # =================================================================
    print("\n--- [Phase 1] LLMì´ í‰ê°€ ê¸°ì¤€í‘œì—ì„œ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜(ì¹´í…Œê³ ë¦¬)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤ ---")
    
    dispatcher_agent = Agent(
        role="í‰ê°€ ê¸°ì¤€í‘œ êµ¬ì¡° ë¶„ì„ ì „ë¬¸ê°€",
        goal="ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í‰ê°€ ê¸°ì¤€í‘œì—ì„œ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜(ë©”ì¸ ì¹´í…Œê³ ë¦¬)ë§Œ ì¶”ì¶œ",
        backstory="""ë‹¹ì‹ ì€ ë³µì¡í•œ ë¬¸ì„œì˜ ê³„ì¸µ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        í‰ê°€ ê¸°ì¤€í‘œì—ì„œ ê°€ì¥ í° ë¶„ë¥˜ ì²´ê³„ë§Œì„ ì •í™•íˆ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.""",
        llm=llm,
        verbose=True
    )

    dispatcher_task = Task(
        description=f"""ì•„ë˜ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í‰ê°€ ê¸°ì¤€í‘œë¥¼ ë¶„ì„í•˜ì—¬ **ìµœìƒìœ„ ëŒ€ë¶„ë¥˜(ë©”ì¸ ì¹´í…Œê³ ë¦¬)**ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

[í‰ê°€ ê¸°ì¤€í‘œ]:
```markdown
{evaluation_criteria_text}
```

ìœ„ í‰ê°€ ê¸°ì¤€í‘œì—ì„œ:
1. ê°€ì¥ í° ë¶„ë¥˜ ì²´ê³„(ìµœìƒìœ„ ëŒ€ë¶„ë¥˜)ë§Œ ì°¾ì•„ì£¼ì„¸ìš”
2. í•˜ìœ„ ì„¸ë¶€ í•­ëª©ë“¤ì€ ë¬´ì‹œí•˜ê³ , í° ì¹´í…Œê³ ë¦¬ì˜ ì´ë¦„ë§Œ ì¶”ì¶œí•˜ì„¸ìš”
3. ì˜ˆë¥¼ ë“¤ì–´ "ê°€ê²© í‰ê°€", "ê¸°ìˆ ë ¥ í‰ê°€", "í”„ë¡œì íŠ¸ ìˆ˜í–‰ ëŠ¥ë ¥" ê°™ì€ ë©”ì¸ ì¹´í…Œê³ ë¦¬ë“¤ì…ë‹ˆë‹¤

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{{
  "categories": [
    {{
      "name": "ëŒ€ë¶„ë¥˜1 ì´ë¦„",
      "description": "ì´ ëŒ€ë¶„ë¥˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…"
    }},
    {{
      "name": "ëŒ€ë¶„ë¥˜2 ì´ë¦„", 
      "description": "ì´ ëŒ€ë¶„ë¥˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…"
    }}
  ]
}}

ì£¼ì˜ì‚¬í•­:
- 'ì†Œê³„', 'ì´ê³„', 'í•©ê³„' ê°™ì€ ê²ƒì€ ì œì™¸í•˜ì„¸ìš”
- ì˜¤ì§ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ë§Œ ì¶”ì¶œí•˜ì„¸ìš” (ë³´í†µ 3-5ê°œ ì •ë„)
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”
""",
        expected_output="ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ì˜ ì´ë¦„ê³¼ ì„¤ëª…ì„ í¬í•¨í•œ JSON ë°°ì—´",
        agent=dispatcher_agent
    )

    dispatcher_crew = Crew(agents=[dispatcher_agent], tasks=[dispatcher_task], verbose=False)
    categorization_result = dispatcher_crew.kickoff()
    
    try:
        raw_result = str(categorization_result.raw)
        # JSON ì¶”ì¶œ
        if "```json" in raw_result:
            start_idx = raw_result.find("```json") + 7
            end_idx = raw_result.find("```", start_idx)
            json_string = raw_result[start_idx:end_idx].strip()
        elif "```" in raw_result:
            start_idx = raw_result.find("```") + 3
            end_idx = raw_result.find("```", start_idx)
            json_string = raw_result[start_idx:end_idx].strip()
        else:
            start_idx = raw_result.find('{')
            end_idx = raw_result.rfind('}') + 1
            json_string = raw_result[start_idx:end_idx]
        
        categories_data = json.loads(json_string)
        main_categories = categories_data.get('categories', [])
        
        print(f"[SUCCESS] LLMì´ ì¶”ì¶œí•œ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ ({len(main_categories)}ê°œ):")
        for cat in main_categories:
            print(f"  - {cat['name']}: {cat.get('description', 'N/A')}")
            
    except (json.JSONDecodeError, ValueError) as e:
        print(f"[ERROR] ëŒ€ë¶„ë¥˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        print(f"   - ì›ë³¸ ê²°ê³¼: {categorization_result.raw}")
        return

    # =================================================================
    # Phase 2: ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ë³„ë¡œë§Œ Agent ìƒì„± (1ê°œ ëŒ€ë¶„ë¥˜ = 1ê°œ Agent)
    # =================================================================
    print(f"\n--- [Phase 2] ì¶”ì¶œëœ {len(main_categories)}ê°œ ëŒ€ë¶„ë¥˜ë³„ë¡œ ì „ë¬¸ê°€ Agentë¥¼ ìƒì„±í•©ë‹ˆë‹¤ ---")
    
    # ëª¨ë“  ì œì•ˆì„œ íŒŒì¼ì— ëŒ€í•´ í‰ê°€ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.
    for proposal_path in proposal_files:
        proposal_name = os.path.basename(proposal_path)
        print(f"\n\n{'='*20} [{proposal_name}] í‰ê°€ ì‹œì‘ {'='*20}")

        specialist_agents = []
        evaluation_tasks = []

        # ê° ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ë§ˆë‹¤ 1ê°œì˜ Agentì™€ 1ê°œì˜ Task ìƒì„±
        for category in main_categories:
            category_name = category['name']
            category_desc = category.get('description', '')
            
            # ëŒ€ë¶„ë¥˜ ì „ë¬¸ê°€ Agent ìƒì„±
            specialist_agent = Agent(
                role=f"'{category_name}' ë¶€ë¬¸ ì „ë¬¸ í‰ê°€ê´€",
                goal=f"'{proposal_name}' ì œì•ˆì„œì˜ '{category_name}' ë¶€ë¬¸ì„ ê°„ê²°í•˜ê²Œ í‰ê°€ (1000ì ì´ë‚´ í•„ìˆ˜)",
                backstory=f"""ë‹¹ì‹ ì€ '{category_name}' ë¶„ì•¼ì˜ ìµœê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                {category_desc}
                
                **ì¤‘ìš”**: ë‹¹ì‹ ì˜ ë³´ê³ ì„œëŠ” ë‹¤ë¥¸ ì‹œìŠ¤í…œì— ì…ë ¥ë˜ë¯€ë¡œ ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
                ê°„ê²°í•˜ê³  í•µì‹¬ë§Œ ë‹´ì€ í‰ê°€ê°€ ìš”êµ¬ë©ë‹ˆë‹¤.""",
                llm=llm,
                verbose=True
            )
            specialist_agents.append(specialist_agent)
            
            # í•´ë‹¹ ëŒ€ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ RAGì—ì„œ ê²€ìƒ‰ (ê°„ê²°í•˜ê²Œ)
            context = get_context_for_category(
                os.path.splitext(proposal_name)[0],
                f"{category_name} {category_desc}"
            )
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½ (í† í° ì ˆì•½)
            if len(context) > 2000:
                context = context[:2000] + "\n...(ì´í•˜ ìƒëµ)"
                print(f"  âš ï¸ '{category_name}' ì»¨í…ìŠ¤íŠ¸ë¥¼ 2000ìë¡œ ì œí•œ")
            
            # ëŒ€ë¶„ë¥˜ ì „ì²´ë¥¼ í‰ê°€í•˜ëŠ” ë‹¨ì¼ Task ìƒì„±
            task = Task(
                description=f"""ì œì•ˆì„œ '{proposal_name}'ì˜ '{category_name}' ë¶€ë¬¸ì„ í‰ê°€í•˜ì„¸ìš”.

**í‰ê°€ ëŒ€ë¶„ë¥˜**: {category_name}

**ì œì•ˆì„œ í•µì‹¬ ë‚´ìš©**:
{context}

**ğŸš¨ğŸš¨ğŸš¨ ì ˆëŒ€ ê·œì¹™: ì „ì²´ ì‘ë‹µì€ ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”! ğŸš¨ğŸš¨ğŸš¨**

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ **ê·¹ë„ë¡œ ê°„ê²°í•˜ê²Œ** ì‘ì„±:

# {category_name} ë¶€ë¬¸

**ë°°ì /ì·¨ë“**: Yì  / Zì  (X%)

**ì£¼ìš” í•­ëª©** (2-3ê°œë§Œ):
- í•­ëª©1 (ë°°ì ): ì ìˆ˜ - 1ì¤„ í‰ê°€
- í•­ëª©2 (ë°°ì ): ì ìˆ˜ - 1ì¤„ í‰ê°€

**ê°•ì **: (2ê°œ, ê° 1ì¤„)
- 
- 

**ì•½ì **: (2ê°œ, ê° 1ì¤„)
- 
- 

**ê°œì„ ì•ˆ**: (1ê°œ, 1ì¤„)
- 

**ì ˆëŒ€ ê·œì¹™**:
- 1000ì ì´ˆê³¼ ì‹œ ì‘ë‹µ ë¬´íš¨
- ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì¼ì²´ ê¸ˆì§€
- í•­ëª©ì€ ìµœëŒ€ 3ê°œë§Œ
- ê° ì„¤ëª…ì€ 1ì¤„ë§Œ
""",
                expected_output=f"'{category_name}' ë¶€ë¬¸ í‰ê°€ (1000ì ì´ë‚´ í•„ìˆ˜)",
                agent=specialist_agent
            )
            evaluation_tasks.append(task)
        
        if not evaluation_tasks:
            print("í‰ê°€í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue

        print(f"\nì´ {len(specialist_agents)}ê°œì˜ ì „ë¬¸ê°€ Agentê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ì´ {len(evaluation_tasks)}ê°œì˜ í‰ê°€ Taskê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

        evaluation_crew = Crew(
            agents=specialist_agents,
            tasks=evaluation_tasks,
            verbose=False
        )
        final_results = await evaluation_crew.kickoff_async()

        print(f"\n--- [Phase 3] [{proposal_name}] ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤ ---")
        
        # kickoff_async()ëŠ” íŠœí”Œì„ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬
        if isinstance(final_results, tuple):
            results_list = final_results[0] if final_results else []
        else:
            results_list = final_results
        
        # ë¶€ë¬¸ë³„ ë³´ê³ ì„œë¥¼ ìˆ˜ì§‘í•˜ê³  ê°•ì œë¡œ ê¸¸ì´ ì œí•œ
        individual_reports = []
        for idx, result in enumerate(results_list):
            if hasattr(result, 'raw'):
                report_text = str(result.raw)
            else:
                report_text = str(result)
            
            # ê°•ì œë¡œ 1000ì ì œí•œ (LLMì´ ë¬´ì‹œí•œ ê²½ìš° ëŒ€ë¹„)
            if len(report_text) > 1000:
                report_text = report_text[:1000] + "\n...(ê¸¸ì´ ì œí•œìœ¼ë¡œ ì ˆì‚­)"
                print(f"  âš ï¸ ë¶€ë¬¸ {idx+1} ë³´ê³ ì„œê°€ 1000ìë¥¼ ì´ˆê³¼í•˜ì—¬ ê°•ì œ ì ˆì‚­")
            
            individual_reports.append(report_text)
        
        individual_reports_text = "\n\n".join(individual_reports)
        
        # í† í° ìˆ˜ í™•ì¸ì„ ìœ„í•œ ì¶œë ¥
        print(f"INFO: ë¶€ë¬¸ë³„ ë³´ê³ ì„œ ì´ ê¸¸ì´: {len(individual_reports_text)}ì")
        
        # ê·¸ë˜ë„ ë„ˆë¬´ ê¸¸ë©´ ì¶”ê°€ ê²½ê³ 
        if len(individual_reports_text) > 4000:
            print(f"  âš ï¸âš ï¸ ê²½ê³ : ë¶€ë¬¸ë³„ ë³´ê³ ì„œ ì´í•©ì´ {len(individual_reports_text)}ìë¡œ ì—¬ì „íˆ ê¹ë‹ˆë‹¤!")
            print(f"  â†’ ê° ë¶€ë¬¸ì„ 500ìë¡œ ì¶”ê°€ ì œí•œí•©ë‹ˆë‹¤.")
            individual_reports = [report[:500] + "..." for report in individual_reports]
            individual_reports_text = "\n\n".join(individual_reports)
            print(f"  â†’ ìµœì¢… ê¸¸ì´: {len(individual_reports_text)}ì")

        reporting_agent = Agent(
            role="ìˆ˜ì„ í‰ê°€ ë¶„ì„ê°€",
            goal="ë¶€ë¬¸ë³„ í‰ê°€ë¥¼ ì¢…í•©í•˜ì—¬ ê²½ì˜ì§„ì´ ì˜ì‚¬ê²°ì •ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ì™„ì„±ëœ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±",
            backstory="""ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ìˆ˜ì„ ë¶„ì„ê°€ë¡œ, í•µì‹¬ì„ íŒŒì•…í•˜ê³  ì „ëµì  ì¸ì‚¬ì´íŠ¸ë¥¼ 
            ì œê³µí•˜ëŠ” ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ë©°, ì˜ì‚¬ê²°ì •ìë“¤ì´ ì‹ ë¢°í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤.""",
            llm=llm, 
            verbose=True
        )
        
        reporting_task = Task(
            description=f"""'{proposal_name}' ì œì•ˆì„œì— ëŒ€í•œ ë¶€ë¬¸ë³„ í‰ê°€ë¥¼ ì¢…í•©í•˜ì—¬ 
ìµœì¢… í‰ê°€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ë¶€ë¬¸ë³„ í‰ê°€ ë³´ê³ ì„œë“¤**:
{individual_reports_text}

**ğŸš¨ ì¤‘ìš”: ìµœì¢… ë³´ê³ ì„œëŠ” 2000ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”!**

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”:

# ì œì•ˆì„œ í‰ê°€ ìµœì¢… ë³´ê³ ì„œ

## 1. Executive Summary (5ì¤„ ì´ë‚´)
- ì œì•ˆì„œ: {proposal_name}
- í‰ê°€ì¼: {datetime.now().strftime('%Y-%m-%d')}
- ì´ì : X/100ì 
- ë“±ê¸‰: S/A/B/C/D
- í•µì‹¬ í‰ê°€: (2ì¤„ ìš”ì•½)

## 2. ë¶€ë¬¸ë³„ ê²°ê³¼ (í‘œ í˜•ì‹)
| ë¶€ë¬¸ | ë°°ì  | ì·¨ë“ | ë¹„ìœ¨ | í‰ê°€ |
|------|------|------|------|------|
| ë¶€ë¬¸1 | XX | YY | ZZ% | 1ì¤„ í‰ê°€ |
| ... |
| ì´ì  | 100 | ì´í•© | í‰ê·  | - |

## 3. ì£¼ìš” ë°œê²¬ì‚¬í•­
**ê°•ì  Top 3** (ê° 1ì¤„):
1. 
2. 
3. 

**ì•½ì  Top 3** (ê° 1ì¤„):
1. 
2. 
3. 

## 4. ê¶Œê³ ì‚¬í•­
**ìš°ì„  ê°œì„ ** (2ê°œ, ê° 1ì¤„):
- 
- 

**ê°•ì  ìœ ì§€** (2ê°œ, ê° 1ì¤„):
- 
- 

## 5. ìµœì¢… ê²°ë¡  (3ì¤„ ì´ë‚´)
- ì¢…í•© ì˜ê²¬:
- ì„ ì • ê¶Œê³ : (ì„ ì • ì¶”ì²œ/ì¡°ê±´ë¶€/íƒˆë½)
- ê·¼ê±°:

**ì‘ì„± ê·œì¹™**:
- ì „ì²´ 2000ì ì´ë‚´ ì—„ìˆ˜
- í‘œì™€ ë¦¬ìŠ¤íŠ¸ í™œìš©í•˜ì—¬ ê°„ê²°í•˜ê²Œ
- ì¤‘ë³µ ë‚´ìš© ì œê±°
- í•µì‹¬ë§Œ í¬í•¨
""",
            expected_output="ê²½ì˜ì§„ ì˜ì‚¬ê²°ì •ìš© ê°„ê²°í•œ ìµœì¢… í‰ê°€ ë³´ê³ ì„œ (2000ì ì´ë‚´)",
            agent=reporting_agent
        )
        
        reporting_crew = Crew(agents=[reporting_agent], tasks=[reporting_task], verbose=False)
        final_comprehensive_report = reporting_crew.kickoff()

        print(f"\n\n[FINAL REPORT] [{proposal_name}] ìµœì¢… ì¢…í•© í‰ê°€ ë³´ê³ ì„œ")
        print("="*80)
        print(final_comprehensive_report.raw)
        print("="*80)
        
        # í‰ê°€ ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥
        save_evaluation_report(proposal_name, final_comprehensive_report.raw)


def run_main():
    """ë™ê¸°ì ìœ¼ë¡œ main í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    asyncio.run(main())

if __name__ == '__main__':
    run_main()