# main_rag.py
# ì œì•ˆì„œ í‰ê°€ë¥¼ ìœ„í•œ í†µí•© RAG ì‹œìŠ¤í…œ
# - ì œì•ˆì„œ, RFP, ì‚¬ë‚´ ì •ë³´ë¥¼ ëª¨ë‘ ì„ë² ë”©í•˜ì—¬ í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
# - CrewAI Agentë“¤ì´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ í™œìš©í•˜ì—¬ ì œì•ˆì„œë¥¼ ìë™ í‰ê°€

import os
import asyncio
import json
import glob
import torch
import chromadb
from dotenv import load_dotenv
from crewai import Agent, Task, Crew
from crewai.llm import LLM

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

load_dotenv()

# =================================================================
# 1. RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ë° í•¨ìˆ˜ ì •ì˜
# =================================================================

# --- ê²½ë¡œ ì„¤ì • ---
PROPOSAL_DIR = "./proposal"  # ì œì•ˆì„œ ë””ë ‰í† ë¦¬
RFP_PATH = "./RFP/ìˆ˜í˜‘_rfp.pdf"  # RFP íŒŒì¼ ê²½ë¡œ
INTERNAL_DATA_DIR = "./internal_data"  # ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬

# --- ì „ì—­ ë³€ìˆ˜ ---
# ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜
# Agentë“¤ì´ ê³µìœ í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
unified_vectorstore = None

def initialize_rag_components():
    """RAGì— í•„ìš”í•œ ì„ë² ë”© ëª¨ë¸ê³¼ ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"INFO: RAG ë””ë°”ì´ìŠ¤ë¡œ '{device}'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    print("INFO: ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    chroma_client = chromadb.PersistentClient(path="./chroma_db_crewai")
    print("INFO: ì„ë² ë”© ëª¨ë¸ ë° ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
    return embedding_model, chroma_client

def load_document(file_path, doc_type, proposal_name):
    """
    ì œì•ˆì„œ/RFP ë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

    Args:
        file_path (str): ë¡œë“œí•  íŒŒì¼ì˜ ê²½ë¡œ
        doc_type (str): ë¬¸ì„œ íƒ€ì… ("RFP" ë˜ëŠ” "ì œì•ˆì„œ")
        proposal_name (str): ì œì•ˆì„œ íŒŒì¼ëª… (RFPì˜ ê²½ìš° "RFP")

    Returns:
        list[Document]: ë¡œë“œëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    documents = []
    print(f"  - [{doc_type}] '{os.path.basename(file_path)}' ë¡œë”© ì¤‘...")

    # PDF íŒŒì¼ ë¡œë“œ (PyPDFLoader ì‚¬ìš©)
    if file_path.endswith('.pdf'):
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        # ê° Documentì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ë¬¸ì„œ íƒ€ì…, ì œì•ˆì„œëª…)
        for doc in docs:
            doc.metadata.update({"doc_type": doc_type, "proposal_name": proposal_name})
        documents.extend(docs)

    # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ (TextLoader ì‚¬ìš©)
    elif file_path.endswith('.txt'):
        loader = TextLoader(file_path, encoding='utf-8')
        docs = loader.load()
        # ê° Documentì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for doc in docs:
            doc.metadata.update({"doc_type": doc_type, "proposal_name": proposal_name})
        documents.extend(docs)

    # HTML íŒŒì¼ ë¡œë“œ (ì¶”ê°€)
    elif file_path.endswith('.html'):
        loader = TextLoader(file_path, encoding='utf-8')
        docs = loader.load()
        for doc in docs:
            doc.metadata.update({"doc_type": doc_type, "proposal_name": proposal_name})
        documents.extend(docs)

    print(f"    â†’ {len(documents)}ê°œ ì„¹ì…˜ ë¡œë“œë¨")
    return documents


# =================================================================
# ì‚¬ë‚´ ì •ë³´ ë¡œë” í•¨ìˆ˜ë“¤
# =================================================================

def load_internal_structured_data(file_path, doc_type_prefix):
    """
    ì •í˜•í™”ëœ ì‚¬ë‚´ ì •ë³´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

    ë°ì´í„° í˜•ì‹: '---'ë¡œ êµ¬ë¶„ëœ í•­ëª©ë“¤
    ê° í•­ëª©ì€ key: value í˜•ì‹ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨

    Args:
        file_path (str): ì‚¬ë‚´ ì •ë³´ íŒŒì¼ ê²½ë¡œ
        doc_type_prefix (str): ë¬¸ì„œ íƒ€ì… ì ‘ë‘ì‚¬ (ì˜ˆ: "ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ")

    Returns:
        list[Document]: ë¡œë“œëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    documents = []

    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not os.path.exists(file_path):
        print(f"  âš  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
        return documents

    print(f"  - [{doc_type_prefix}] '{os.path.basename(file_path)}' ë¡œë”© ì¤‘...")

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # '---'ë¡œ êµ¬ë¶„ëœ ê° í•­ëª©ì„ ê°œë³„ Documentë¡œ ë³€í™˜
    entries = content.split('---')

    for entry in entries:
        entry = entry.strip()
        if not entry:  # ë¹ˆ í•­ëª©ì€ ìŠ¤í‚µ
            continue

        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
        metadata = {"doc_type": doc_type_prefix}

        # ê° ì¤„ì„ íŒŒì‹±í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        # í˜•ì‹: key: value
        lines = entry.split('\n')
        for line in lines:
            if ':' in line:
                key, value = line.split(':', 1)
                # ë©”íƒ€ë°ì´í„°ì— í‚¤-ê°’ ì €ì¥ (ê³µë°± ì œê±°)
                metadata[key.strip()] = value.strip()

        # Document ìƒì„± (ì „ì²´ ë‚´ìš©ì„ page_contentë¡œ, íŒŒì‹±ëœ ì •ë³´ëŠ” metadataë¡œ)
        doc = Document(
            page_content=entry,  # ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì €ì¥
            metadata=metadata
        )
        documents.append(doc)

    print(f"    â†’ {len(documents)}ê°œ í•­ëª© ë¡œë“œë¨")
    return documents


def load_all_internal_data(internal_data_dir):
    """
    ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ìë™ìœ¼ë¡œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

    íŒŒì¼ëª… ê·œì¹™:
    - tech_stacks*.txt â†’ ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ
    - contacts*.txt â†’ ì‚¬ë‚´_ë‹´ë‹¹ì
    - migrations*.txt â†’ ì‚¬ë‚´_ë§ˆì´ê·¸ë ˆì´ì…˜
    - incidents*.txt â†’ ì‚¬ë‚´_ì¥ì• ì´ë ¥

    Args:
        internal_data_dir (str): ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        list[Document]: ëª¨ë“  ì‚¬ë‚´ ì •ë³´ Document ë¦¬ìŠ¤íŠ¸
    """
    all_internal_docs = []

    if not os.path.exists(internal_data_dir):
        print(f"  âš  ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {internal_data_dir}")
        return all_internal_docs

    print(f"\n[ì‚¬ë‚´ ì •ë³´ ë¡œë“œ ì‹œì‘: {internal_data_dir}]")

    # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  .txt íŒŒì¼ ê²€ìƒ‰
    internal_files = glob.glob(os.path.join(internal_data_dir, "*.txt"))

    for file_path in internal_files:
        filename = os.path.basename(file_path).lower()

        # íŒŒì¼ëª…ì— ë”°ë¼ ë¬¸ì„œ íƒ€ì… ìë™ ë¶„ë¥˜
        if 'tech_stack' in filename or 'technology' in filename:
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ")
            all_internal_docs.extend(docs)

        elif 'contact' in filename or 'person' in filename:
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ë‹´ë‹¹ì")
            all_internal_docs.extend(docs)

        elif 'migration' in filename:
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ë§ˆì´ê·¸ë ˆì´ì…˜")
            all_internal_docs.extend(docs)

        elif 'incident' in filename or 'failure' in filename:
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ì¥ì• ì´ë ¥")
            all_internal_docs.extend(docs)

        else:
            # ë¶„ë¥˜ë˜ì§€ ì•Šì€ íŒŒì¼ì€ ì¼ë°˜ ì‚¬ë‚´ ì •ë³´ë¡œ ì²˜ë¦¬
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ê¸°íƒ€")
            all_internal_docs.extend(docs)

    print(f"âœ… ì´ {len(all_internal_docs)}ê°œì˜ ì‚¬ë‚´ ì •ë³´ í•­ëª© ë¡œë“œ ì™„ë£Œ\n")
    return all_internal_docs

def create_unified_vectorstore(
    proposal_files,
    rfp_path,
    internal_data_dir,
    embedding_model,
    chroma_client,
    collection_name="proposal_evaluation_store"
):
    """
    ì œì•ˆì„œ, RFP, ì‚¬ë‚´ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” í†µí•© ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

    Args:
        proposal_files (list): ì œì•ˆì„œ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        rfp_path (str): RFP íŒŒì¼ ê²½ë¡œ
        internal_data_dir (str): ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        embedding_model: HuggingFace ì„ë² ë”© ëª¨ë¸
        chroma_client: ChromaDB í´ë¼ì´ì–¸íŠ¸
        collection_name (str): ë²¡í„°ìŠ¤í† ì–´ ì»¬ë ‰ì…˜ ì´ë¦„

    Returns:
        Chroma: ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
    """
    print(f"\n{'='*70}")
    print(f"  í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹œì‘ (Collection: {collection_name})")
    print(f"{'='*70}")

    all_documents = []

    # 1. RFP ë¬¸ì„œ ë¡œë“œ
    print("\n[1ë‹¨ê³„] RFP ë¬¸ì„œ ë¡œë“œ")
    if os.path.exists(rfp_path):
        all_documents.extend(load_document(rfp_path, "RFP", "RFP"))
    else:
        print(f"  âš  RFP íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {rfp_path}")

    # 2. ì œì•ˆì„œ ë¬¸ì„œ ë¡œë“œ
    print("\n[2ë‹¨ê³„] ì œì•ˆì„œ ë¬¸ì„œ ë¡œë“œ")
    for proposal_path in proposal_files:
        proposal_name = os.path.basename(proposal_path)
        all_documents.extend(load_document(proposal_path, "ì œì•ˆì„œ", proposal_name))

    # 3. ì‚¬ë‚´ ì •ë³´ ë¡œë“œ (í•µì‹¬ ì¶”ê°€ ë¶€ë¶„!)
    print("\n[3ë‹¨ê³„] ì‚¬ë‚´ ì •ë³´ ë¡œë“œ")
    internal_docs = load_all_internal_data(internal_data_dir)
    all_documents.extend(internal_docs)

    # ì „ì²´ ë¡œë“œ ì™„ë£Œ í†µê³„
    print(f"\n{'='*70}")
    print(f"  âœ… ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ì„¹ì…˜ ë¡œë“œ ì™„ë£Œ")
    print(f"{'='*70}")

    # 4. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
    # - ì œì•ˆì„œ/RFP: ê¸´ ë¬¸ì„œì´ë¯€ë¡œ ì²­í¬ ë¶„í•  í•„ìš”
    # - ì‚¬ë‚´ ì •ë³´: ì´ë¯¸ ì‘ì€ ë‹¨ìœ„ì´ì§€ë§Œ í†µì¼ì„±ì„ ìœ„í•´ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    print("\n[4ë‹¨ê³„] í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• ")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,      # ì²­í¬ í¬ê¸°: 500ì (ë¬¸ë§¥ ìœ ì§€)
        chunk_overlap=50,    # ì²­í¬ ê°„ ì˜¤ë²„ë©: 50ì (ê²½ê³„ ì •ë³´ ìœ ì§€)
        separators=["\n\n", "\n", ". ", " ", ""]  # ë¶„í•  ìš°ì„ ìˆœìœ„
    )
    splits = text_splitter.split_documents(all_documents)
    print(f"  âœ… {len(splits)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

    # 5. ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ìˆì„ ê²½ìš°)
    print("\n[5ë‹¨ê³„] ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸ ë° ì‚­ì œ")
    try:
        chroma_client.delete_collection(name=collection_name)
        print(f"  âœ… ê¸°ì¡´ '{collection_name}' ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
    except Exception:
        print(f"  â„¹ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì—†ìŒ (ì‹ ê·œ ìƒì„±)")

    # 6. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì„ë² ë”© + ì¸ë±ì‹±)
    print("\n[6ë‹¨ê³„] ë²¡í„° ì„ë² ë”© ë° ì¸ë±ì‹±")
    print(f"  â³ {len(splits)}ê°œ ì²­í¬ë¥¼ ì„ë² ë”© ì¤‘... (ìˆ˜ ë¶„ ì†Œìš”)")
    vectorstore = Chroma.from_documents(
        documents=splits,              # ì²­í¬ ë¶„í• ëœ Document ë¦¬ìŠ¤íŠ¸
        embedding=embedding_model,     # ì„ë² ë”© ëª¨ë¸
        collection_name=collection_name,  # ì»¬ë ‰ì…˜ ì´ë¦„
        client=chroma_client          # ChromaDB í´ë¼ì´ì–¸íŠ¸
    )

    print(f"\n{'='*70}")
    print(f"  ğŸ‰ í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
    print(f"  - ì´ ì²­í¬ ìˆ˜: {len(splits)}ê°œ")
    print(f"  - ì»¬ë ‰ì…˜ëª…: {collection_name}")
    print(f"{'='*70}\n")

    return vectorstore

def get_context_for_topic(proposal_file, topic):
    """
    ì œì•ˆì„œì—ì„œ íŠ¹ì • í† í”½ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜

    Args:
        proposal_file (str): ì œì•ˆì„œ íŒŒì¼ëª… (ì˜ˆ: "Aì‚¬_ì œì•ˆì„œ.txt")
        topic (str): ê²€ìƒ‰í•  í† í”½ (ì˜ˆ: "ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜")

    Returns:
        str: ê²€ìƒ‰ëœ ê´€ë ¨ ë‚´ìš© (ì—¬ëŸ¬ ì²­í¬ë¥¼ í•©ì¹œ ë¬¸ìì—´)
    """
    global unified_vectorstore

    # ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
    if unified_vectorstore is None:
        return "ì˜¤ë¥˜: ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    print(f"  ğŸ” RAG ê²€ìƒ‰ ì‹¤í–‰ -> ì œì•ˆì„œ: '{proposal_file}', í† í”½: '{topic}'")

    # ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
    # - query: ê²€ìƒ‰ ì¿¼ë¦¬ (í† í”½)
    # - k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜ (ìƒìœ„ 2ê°œ)
    # - filter: ë©”íƒ€ë°ì´í„° í•„í„°ë§ (íŠ¹ì • ì œì•ˆì„œë§Œ ê²€ìƒ‰)
    results = unified_vectorstore.similarity_search(
        query=topic,
        k=2,  # ê´€ë ¨ì„±ì´ ë†’ì€ ìƒìœ„ 2ê°œ ì²­í¬ë§Œ ê°€ì ¸ì˜´
        filter={"proposal_name": proposal_file}  # íŠ¹ì • ì œì•ˆì„œë¡œ í•„í„°ë§
    )

    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    if not results:
        return "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ê²€ìƒ‰ëœ ì—¬ëŸ¬ ì²­í¬ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
    context = "\n\n---\n\n".join([doc.page_content for doc in results])
    return context


def search_internal_info(query, doc_type=None, k=3):
    """
    ì‚¬ë‚´ ì •ë³´ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    (ì‚¬ìš©ì ì§ˆë¬¸: "íƒ€ ê³„ì—´ì‚¬ì—ì„œ Kafka ì“°ëŠ” ê³³ ìˆë‚˜?" ë“±ì— ì‚¬ìš©)

    Args:
        query (str): ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "Kafka ì¥ì•  ì´ë ¥")
        doc_type (str, optional): ë¬¸ì„œ íƒ€ì… í•„í„° (ì˜ˆ: "ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ")
        k (int): ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ê°’: 3)

    Returns:
        list[Document]: ê²€ìƒ‰ëœ Document ë¦¬ìŠ¤íŠ¸
    """
    global unified_vectorstore

    if unified_vectorstore is None:
        print("âš  ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    print(f"  ğŸ” ì‚¬ë‚´ ì •ë³´ ê²€ìƒ‰ -> ì¿¼ë¦¬: '{query}', íƒ€ì…: '{doc_type or 'ì „ì²´'}'")

    # ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„±
    filter_dict = {}
    if doc_type:
        filter_dict["doc_type"] = doc_type

    # ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰
    results = unified_vectorstore.similarity_search(
        query=query,
        k=k,
        filter=filter_dict if filter_dict else None
    )

    print(f"  âœ… {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
    return results


# =================================================================
# ì‚¬ë‚´ ì •ë³´ ê²€ìƒ‰ í—¬í¼ í•¨ìˆ˜ë“¤ (ì„ íƒì  ì‚¬ìš©)
# =================================================================

def search_tech_stack(technology_name, k=3):
    """íŠ¹ì • ê¸°ìˆ  ìŠ¤íƒ ì •ë³´ ê²€ìƒ‰"""
    return search_internal_info(technology_name, doc_type="ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ", k=k)


def search_contact(query, k=3):
    """ë‹´ë‹¹ì ì •ë³´ ê²€ìƒ‰"""
    return search_internal_info(query, doc_type="ì‚¬ë‚´_ë‹´ë‹¹ì", k=k)


def search_migration_case(technology_name, k=2):
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ì‚¬ë¡€ ê²€ìƒ‰"""
    return search_internal_info(f"{technology_name} ë§ˆì´ê·¸ë ˆì´ì…˜", doc_type="ì‚¬ë‚´_ë§ˆì´ê·¸ë ˆì´ì…˜", k=k)


def search_incident(technology_name, k=2):
    """ì¥ì•  ì´ë ¥ ê²€ìƒ‰"""
    return search_internal_info(f"{technology_name} ì¥ì• ", doc_type="ì‚¬ë‚´_ì¥ì• ì´ë ¥", k=k)


#==============================================================


# 2. CrewAI Agent ë° í”„ë¡œì„¸ìŠ¤ ì •ì˜
# =================================================================

async def main():
    """
    ë©”ì¸ í•¨ìˆ˜: ì œì•ˆì„œ ìë™ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰

    ì „ì²´ íë¦„:
    1. RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ì„ë² ë”© ëª¨ë¸, ë²¡í„°ìŠ¤í† ì–´)
    2. Phase 1: ì‹¬ì‚¬ í•­ëª© ìë™ ë¶„ë¥˜ (Dispatcher Agent)
    3. Phase 2: ëŒ€ë¶„ë¥˜ë³„ ì „ë¬¸ê°€ Agentê°€ ë³‘ë ¬ í‰ê°€
    4. Phase 3: ìµœì¢… ë³´ê³ ì„œ ì‘ì„± (Reporting Agent)
    """
    print("\n" + "="*70)
    print("  ë™ì  Agent ìƒì„± ë° í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("="*70)

    # --- [ì „ì œ] RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ---
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì „, ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¨¼ì € ì¤€ë¹„í•©ë‹ˆë‹¤.
    global unified_vectorstore

    # ì œì•ˆì„œ íŒŒì¼ ê²€ìƒ‰ (.txt, .html ë“±)
    proposal_files = glob.glob(os.path.join(PROPOSAL_DIR, "*.txt"))
    proposal_files.extend(glob.glob(os.path.join(PROPOSAL_DIR, "*.html")))

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not proposal_files:
        print("âŒ ì˜¤ë¥˜: ì œì•ˆì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   - ì œì•ˆì„œ ë””ë ‰í† ë¦¬: {PROPOSAL_DIR}")
        return

    if not os.path.exists(RFP_PATH):
        print("âŒ ì˜¤ë¥˜: RFP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   - RFP ê²½ë¡œ: {RFP_PATH}")
        return

    print(f"\nâœ… ì œì•ˆì„œ íŒŒì¼ {len(proposal_files)}ê°œ ë°œê²¬:")
    for pf in proposal_files:
        print(f"   - {os.path.basename(pf)}")

    # RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ì„ë² ë”© ëª¨ë¸, ChromaDB)
    embedding_model, chroma_client = initialize_rag_components()

    # í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì œì•ˆì„œ + RFP + ì‚¬ë‚´ì •ë³´)
    unified_vectorstore = create_unified_vectorstore(
        proposal_files=proposal_files,
        rfp_path=RFP_PATH,
        internal_data_dir=INTERNAL_DATA_DIR,  # ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ ì¶”ê°€!
        embedding_model=embedding_model,
        chroma_client=chroma_client
    )
    # --- RAG ì´ˆê¸°í™” ì™„ë£Œ ---

    # ì „ì²´ ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ (ë¹„ì •í˜• ë°ì´í„°)
    # ì‹¤ì œë¡œëŠ” ì‹¬ì‚¬ê¸°ì¤€í‘œ íŒŒì¼ì—ì„œ ë¡œë“œí•  ìˆ˜ ìˆìŒ
    unstructured_evaluation_items = [
        {"ëŒ€ë¶„ë¥˜": "ê¸°ìˆ ", "topic": "ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜", "criteria": "MSA ê¸°ë°˜ì˜ ìœ ì—°í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ì¸ê°€?"},
        {"ëŒ€ë¶„ë¥˜": "ê´€ë¦¬", "topic": "í”„ë¡œì íŠ¸ ê´€ë¦¬ ë°©ì•ˆ", "criteria": "WBS ê¸°ë°˜ì˜ ìƒì„¸í•˜ê³  ì‹¤í˜„ ê°€ëŠ¥í•œ ì¼ì •ì„ ì œì‹œí•˜ì˜€ëŠ”ê°€?"},
        {"ëŒ€ë¶„ë¥˜": "ê¸°ìˆ ", "topic": "ë°ì´í„°ë² ì´ìŠ¤ ì•”í˜¸í™”", "criteria": "ê°œì¸ì •ë³´ë³´í˜¸ ë° ë°ì´í„° ì•”í˜¸í™” ë°©ì•ˆì´ ëª…ì‹œë˜ì—ˆëŠ”ê°€?"},
        {"ëŒ€ë¶„ë¥˜": "ê´€ë¦¬", "topic": "íˆ¬ì… ì¸ë ¥ ê³„íš", "criteria": "íˆ¬ì… ì¸ë ¥ì˜ ì—­í• ê³¼ ê²½ë ¥ì´ ì ì ˆí•œê°€?"},
        {"ëŒ€ë¶„ë¥˜": "ê°€ê²©", "topic": "ë¹„ìš© ì‚°ì • ë‚´ì—­", "criteria": "ì œì‹œëœ ë¹„ìš©ì´ í•©ë¦¬ì ì´ê³  êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ í¬í•¨í•˜ëŠ”ê°€?"},
    ]

    # --- LLM ì •ì˜ ---
    # Ollama ë¡œì»¬ ì„œë²„ ì‚¬ìš© (llama3.2 ëª¨ë¸)
    # ë˜ëŠ” HuggingFace Inference API ì‚¬ìš© ê°€ëŠ¥
    llm = LLM(model="ollama/llama3.2", base_url="http://localhost:11434")

    # =================================================================
    # Phase 1: Dispatcherê°€ ëŒ€ë¶„ë¥˜ë¥¼ ìŠ¤ìŠ¤ë¡œ ì°¾ì•„ë‚´ê³  í•­ëª© ë¶„ë¥˜
    # =================================================================
    # ëª©ì : ë¹„ì •í˜• ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ 'ëŒ€ë¶„ë¥˜' ê¸°ì¤€ìœ¼ë¡œ ìë™ ê·¸ë£¹í™”
    # ì˜ˆ: {"ê¸°ìˆ ": [...], "ê´€ë¦¬": [...], "ê°€ê²©": [...]}
    print("\n" + "="*70)
    print("  [Phase 1] ì‹¬ì‚¬ í•­ëª© ìë™ ë¶„ë¥˜")
    print("="*70)
    
    dispatcher_agent = Agent(
        role="í‰ê°€ í•­ëª© ìë™ ë¶„ë¥˜ ë° ê·¸ë£¹í™” ì „ë¬¸ê°€",
        goal="ì£¼ì–´ì§„ ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ì—ì„œ 'ëŒ€ë¶„ë¥˜'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  í•­ëª©ì„ ê·¸ë£¹í™”í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜",
        backstory="ë‹¹ì‹ ì€ ë³µì¡í•œ ëª©ë¡ì„ ë°›ì•„ì„œ ì£¼ìš” ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ê³  êµ¬ì¡°í™”í•˜ëŠ” ë° ë§¤ìš° ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ê°€ì¡ŒìŠµë‹ˆë‹¤.",
        llm=llm,
        verbose=True
    )

    items_as_string = json.dumps(unstructured_evaluation_items, ensure_ascii=False)
    
    dispatcher_task = Task(
        description=f"""ì•„ë˜ ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ 'ëŒ€ë¶„ë¥˜' í‚¤ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•´ì£¼ì„¸ìš”.
        [ì „ì²´ ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸]: {items_as_string}
        ê²°ê³¼ JSONì˜ keyëŠ” ë¦¬ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ëŠ” 'ëŒ€ë¶„ë¥˜'ì˜ ì´ë¦„ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        ê° í•­ëª©ì˜ 'ëŒ€ë¶„ë¥˜', 'topic', 'criteria' í‚¤ì™€ ê°’ì„ ëª¨ë‘ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.
        """,
        expected_output="JSON ê°ì²´. ê° keyëŠ” ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ì— ìˆë˜ 'ëŒ€ë¶„ë¥˜'ì´ë©°, valueëŠ” í•´ë‹¹ ëŒ€ë¶„ë¥˜ì— ì†í•˜ëŠ” í•­ëª© ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ê° ê°ì²´ëŠ” ì›ë³¸ì˜ ëª¨ë“  í‚¤-ê°’ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.",
        agent=dispatcher_agent
    )

    dispatcher_crew = Crew(agents=[dispatcher_agent], tasks=[dispatcher_task], verbose=False)
    categorization_result = dispatcher_crew.kickoff()
    
    try:
        # LLMì´ ìƒì„±í•œ ê²°ê³¼ë¬¼ì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_string = categorization_result.raw[categorization_result.raw.find('{'):categorization_result.raw.rfind('}')+1]
        categorized_items = json.loads(json_string)
        print("âœ… í•­ëª© ë¶„ë¥˜ ì™„ë£Œ. ë°œê²¬ëœ ëŒ€ë¶„ë¥˜:")
        for category, items in categorized_items.items():
            print(f"  - {category}: {len(items)}ê°œ í•­ëª©")
    except (json.JSONDecodeError, IndexError):
        print("âŒ í•­ëª© ë¶„ë¥˜ ì‹¤íŒ¨!")
        print(f"   - ì›ë³¸ ê²°ê³¼: {categorization_result.raw}")
        categorized_items = {}

    # =================================================================
    # Phase 2: ëŒ€ë¶„ë¥˜ ê°œìˆ˜ë§Œí¼ ë™ì ìœ¼ë¡œ Agentë¥¼ ìƒì„±í•˜ê³  ë³‘ë ¬ í‰ê°€
    # =================================================================
    # ëª©ì :
    # - ê° ëŒ€ë¶„ë¥˜(ê¸°ìˆ , ê´€ë¦¬, ê°€ê²© ë“±)ë³„ë¡œ ì „ë¬¸ê°€ Agentë¥¼ ë™ì  ìƒì„±
    # - ê° ì œì•ˆì„œë¥¼ ìˆœíšŒí•˜ë©° ëª¨ë“  ì‹¬ì‚¬ í•­ëª©ì„ ë³‘ë ¬ í‰ê°€
    # - RAGë¥¼ í†µí•´ ì œì•ˆì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ìë™ ì¶”ì¶œí•˜ì—¬ í‰ê°€ ê·¼ê±°ë¡œ í™œìš©
    print("\n" + "="*70)
    print("  [Phase 2] ë°œê²¬ëœ ëŒ€ë¶„ë¥˜ë³„ë¡œ ì „ë¬¸ê°€ Agentë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ë³‘ë ¬ í‰ê°€í•©ë‹ˆë‹¤")
    print("="*70)

    # ëª¨ë“  ì œì•ˆì„œ íŒŒì¼ì— ëŒ€í•´ í‰ê°€ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.
    for proposal_path in proposal_files:
        proposal_name = os.path.basename(proposal_path)
        print(f"\n\n{'='*20} [{proposal_name}] í‰ê°€ ì‹œì‘ {'='*20}")

        # ì œì•ˆì„œë³„ë¡œ Agentì™€ Task ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        specialist_agents = []  # ì „ë¬¸ê°€ Agent ë¦¬ìŠ¤íŠ¸
        evaluation_tasks = []   # í‰ê°€ Task ë¦¬ìŠ¤íŠ¸

        # ëŒ€ë¶„ë¥˜ë³„ë¡œ ì „ë¬¸ê°€ Agentë¥¼ ë™ì  ìƒì„±
        for category, items in categorized_items.items():
            # ëŒ€ë¶„ë¥˜ë³„ ì „ë¬¸ê°€ Agent ìƒì„± (ì˜ˆ: "ê¸°ìˆ  ë¶€ë¬¸ ì „ë¬¸ í‰ê°€ê´€")
            specialist_agent = Agent(
                role=f"'{category}' ë¶€ë¬¸ ì „ë¬¸ í‰ê°€ê´€",
                goal=f"'{proposal_name}' ì œì•ˆì„œì˜ '{category}' ë¶€ë¬¸ì„ ì „ë¬¸ì ìœ¼ë¡œ í‰ê°€",
                backstory=f"ë‹¹ì‹ ì€ '{category}' ë¶„ì•¼ ìµœê³ ì˜ ì „ë¬¸ê°€ë¡œì„œ, ì£¼ì–´ì§„ ê´€ë ¨ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì‚¬ ê¸°ì¤€ì— ë”°ë¼ ì œì•ˆì„œë¥¼ ëƒ‰ì² í•˜ê²Œ ë¶„ì„í•˜ê³  í‰ê°€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.",
                llm=llm,
                verbose=True
            )
            specialist_agents.append(specialist_agent)

            # í•´ë‹¹ ëŒ€ë¶„ë¥˜ì˜ ëª¨ë“  ì‹¬ì‚¬ í•­ëª©ì— ëŒ€í•œ Task ìƒì„±
            for item in items:
                # RAGë¥¼ í†µí•´ ì œì•ˆì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰
                # - ë²¡í„°ìŠ¤í† ì–´ì—ì„œ í† í”½ê³¼ ìœ ì‚¬í•œ ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì˜´
                context = get_context_for_topic(proposal_name, item['topic'])

                # í‰ê°€ Task ìƒì„±
                task = Task(
                    description=f"""'{proposal_name}' ì œì•ˆì„œì˜ '{category}' ë¶€ë¬¸ ì¤‘ '{item['topic']}' í•­ëª©ì„ í‰ê°€í•˜ì‹œì˜¤.

- ì‹¬ì‚¬ ê¸°ì¤€: {item['criteria']}

- ì œì•ˆì„œ ê´€ë ¨ ë‚´ìš©:
---
{context}
---

ìœ„ ë‚´ìš©ì„ ê·¼ê±°ë¡œ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì‹œì˜¤.""",
                    expected_output=f"'{item['topic']}' í•­ëª©ì— ëŒ€í•œ í‰ê°€ ë³´ê³ ì„œ. ë°˜ë“œì‹œ [í‰ê°€ ì ìˆ˜(1-100)], [í‰ê°€ ìš”ì•½], [íŒë‹¨ ê·¼ê±°] ì„¸ ê°€ì§€ í•­ëª©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.",
                    agent=specialist_agent  # í•´ë‹¹ ëŒ€ë¶„ë¥˜ ì „ë¬¸ê°€ì—ê²Œ í• ë‹¹
                )
                evaluation_tasks.append(task)

        # Taskê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ì œì•ˆì„œë¡œ
        if not evaluation_tasks:
            print("âš  í‰ê°€í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # Crew êµ¬ì„± ë° ë³‘ë ¬ í‰ê°€ ì‹¤í–‰
        # - ì—¬ëŸ¬ ì „ë¬¸ê°€ Agentê°€ ê°ìì˜ Taskë¥¼ ë™ì‹œì— ìˆ˜í–‰
        print(f"\nâ³ {len(evaluation_tasks)}ê°œ í‰ê°€ í•­ëª©ì„ ì²˜ë¦¬ ì¤‘...")
        evaluation_crew = Crew(
            agents=specialist_agents,  # ë™ì ìœ¼ë¡œ ìƒì„±ëœ ì „ë¬¸ê°€ Agent ë¦¬ìŠ¤íŠ¸
            tasks=evaluation_tasks,    # ë™ì ìœ¼ë¡œ ìƒì„±ëœ Task ë¦¬ìŠ¤íŠ¸
            verbose=True
        )
        final_results = await evaluation_crew.kickoff_async()  # ë¹„ë™ê¸° ë³‘ë ¬ ì‹¤í–‰

        # =================================================================
        # Phase 3: ìµœì¢… ë³´ê³ ì„œ ì‘ì„± (Reporting Agent)
        # =================================================================
        # ëª©ì :
        # - ëª¨ë“  ê°œë³„ í‰ê°€ ë³´ê³ ì„œë¥¼ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±
        # - ì œì•ˆì„œ ì „ì²´ì— ëŒ€í•œ ì¢…í•© í‰ê°€, ê°•ì /ì•½ì  ë¶„ì„, ìµœì¢… ì ìˆ˜ ì œì‹œ
        print(f"\n{'='*70}")
        print(f"  [Phase 3] [{proposal_name}] ìµœì¢… ë³´ê³ ì„œ ì‘ì„±")
        print(f"{'='*70}")

        # ê°œë³„ í‰ê°€ ë³´ê³ ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
        individual_reports = "\n\n".join([str(result) for result in final_results])

        # ìµœì¢… ë³´ê³ ì„œ ì‘ì„± Agent ìƒì„±
        reporting_agent = Agent(
            role="ìˆ˜ì„ í‰ê°€ ë¶„ì„ê°€",
            goal="ì—¬ëŸ¬ ê°œì˜ ê°œë³„ í‰ê°€ ë³´ê³ ì„œë¥¼ ì¢…í•©í•˜ì—¬, ê²½ì˜ì§„ì´ ì˜ì‚¬ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ í•˜ë‚˜ì˜ ì™„ì„±ëœ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±",
            backstory="ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë¶€ì„œì˜ ë³´ê³ ë¥¼ ì·¨í•©í•˜ì—¬ í•µì‹¬ë§Œ ìš”ì•½í•˜ê³ , ì „ì²´ì ì¸ ê´€ì ì—ì„œ ê°•ì ê³¼ ì•½ì ì„ ë¶„ì„í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë° ë§¤ìš° ëŠ¥ìˆ™í•©ë‹ˆë‹¤.",
            llm=llm,
            verbose=True
        )

        # ìµœì¢… ë³´ê³ ì„œ ì‘ì„± Task ìƒì„±
        reporting_task = Task(
            description=f"""ì•„ë˜ëŠ” '{proposal_name}' ì œì•ˆì„œì— ëŒ€í•œ ê° ë¶„ì•¼ ì „ë¬¸ê°€ë“¤ì˜ ê°œë³„ í‰ê°€ ë³´ê³ ì„œì…ë‹ˆë‹¤.

[ê°œë³„ í‰ê°€ ë³´ê³ ì„œ ëª©ë¡]
{individual_reports}

ìœ„ ë³´ê³ ì„œë“¤ì„ ëª¨ë‘ ì¢…í•©í•˜ì—¬, ì œì•ˆì„œ ì „ì²´ì— ëŒ€í•œ ìµœì¢… í‰ê°€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë³´ê³ ì„œëŠ” ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
- [ì´í‰]: ì œì•ˆì„œ ì „ì²´ì— ëŒ€í•œ ì¢…í•© í‰ê°€
- [ì£¼ìš” ê°•ì ]: ì œì•ˆì„œì˜ ë›°ì–´ë‚œ ì  (ìƒìœ„ 3ê°œ)
- [ì£¼ìš” ì•½ì ]: ì œì•ˆì„œì˜ ë¶€ì¡±í•œ ì  (ìƒìœ„ 3ê°œ)
- [ìµœì¢… ì¶”ì²œ ì ìˆ˜(1-100)]: ì¢…í•© ì ìˆ˜ ë° ì„ ì • ì¶”ì²œ ì—¬ë¶€""",
            expected_output="í•˜ë‚˜ì˜ ì™„ì„±ëœ ìµœì¢… í‰ê°€ ë³´ê³ ì„œ.",
            agent=reporting_agent
        )

        # ìµœì¢… ë³´ê³ ì„œ ìƒì„± Crew ì‹¤í–‰
        reporting_crew = Crew(agents=[reporting_agent], tasks=[reporting_task], verbose=False)
        final_comprehensive_report = reporting_crew.kickoff()

        # ìµœì¢… ë³´ê³ ì„œ ì¶œë ¥
        print(f"\n\n{'='*70}")
        print(f"  ğŸ‰ [{proposal_name}] ìµœì¢… ì¢…í•© í‰ê°€ ë³´ê³ ì„œ")
        print(f"{'='*70}")
        print(final_comprehensive_report.raw)
        print(f"{'='*70}\n")


# =================================================================
# í”„ë¡œê·¸ë¨ ì‹¤í–‰
# =================================================================

if __name__ == '__main__':
    """
    ì‚¬ìš© ë°©ë²•:
    1. ë””ë ‰í† ë¦¬ êµ¬ì¡° ì¤€ë¹„:
       - ./proposal/        : ì œì•ˆì„œ íŒŒì¼ë“¤ (.txt, .html)
       - ./RFP/            : RFP íŒŒì¼ (ìˆ˜í˜‘_rfp.pdf)
       - ./internal_data/  : ì‚¬ë‚´ ì •ë³´ íŒŒì¼ë“¤ (.txt)

    2. Ollama ì„œë²„ ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„):
       $ ollama serve
       $ ollama run llama3.2

    3. í”„ë¡œê·¸ë¨ ì‹¤í–‰:
       $ python main_rag.py

    4. ê²°ê³¼ í™•ì¸:
       - ì½˜ì†”ì— ìµœì¢… í‰ê°€ ë³´ê³ ì„œ ì¶œë ¥
       - ChromaDBì— ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ (./chroma_db_crewai/)
    """
    asyncio.run(main())